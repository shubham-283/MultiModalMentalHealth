# ğŸ§  MultiModalMentalHealth

<div align="center">

<img src="ResultsOfProject/web1.png" alt="Hero Image" width="400"/>
<img src="ResultsOfProject/web2.png" alt="Hero Image" width="400"/>

<h3>An AI-Powered Mental Health Assessment System</h3>
<p>Comprehensive emotional well-being analysis through <b>text</b>, <b>audio</b>, and <b>video</b> detection</p>

<!-- Multiple images displayed in a neat row -->
<img src="ResultsOfProject/web3.png" alt="Text Analysis" width="400" style="margin: 10px"/>
<img src="ResultsOfProject/web4.png" alt="Audio Analysis" width="400" style="margin: 10px"/>
<img src="ResultsOfProject/web5.png" alt="Video Analysis" width="400" style="margin: 10px"/>

</div>

---

## ğŸ¯ Key Features

### ğŸ”¤ Text Emotion Analysis
- ğŸ’¬ Understand users' emotions through their words using advanced BERT-based NLP models
- ğŸ“ˆ Perform real-time sentiment analysis from written messages, journal entries, or social posts
- ğŸ§  Detect potential mental health concerns with professional-grade precision

### ğŸµ Audio Emotion Recognition
- ğŸ™ï¸ Analyze tone, pitch, and voice patterns using Librosa & TensorFlow
- âš¡ Get instant feedback with real-time speech emotion detection
- ğŸ§ Smart preprocessing and feature extraction ensure robust performance

### ğŸ“¹ Video Facial Expression Detection
- ğŸ‘€ Capture facial cues using YOLOv8-powered computer vision
- ğŸ•µï¸â€â™€ï¸ Detect micro-expressions and multi-emotion blends with confidence scoring
- ğŸ¥ Supports live webcam analysis for dynamic emotional tracking

### ğŸ“Š Happiness Meter Visualization
- ğŸ§­ Speedometer-style indicators offer a quick snapshot of emotional well-being
- ğŸŸ¢ ğŸŸ¡ ğŸ”´ Color-coded emotional states:
  - **Green** = Happy
  - **Yellow** = Neutral
  - **Red** = Distressed
- ğŸ“Š A unified multimodal dashboard brings together all insights in a human-centered way

---

## ğŸ–¼ï¸ Application Interface

A single-page application that brings together all emotional analysis tools in one seamless experience.

<p align="center">
  <img src="ResultsOfProject/web1.png" alt="Main Dashboard Full View" width="800"/>
</p>

<p align="center">
  <img src="ResultsOfProject/web2.png" alt="Main Dashboard Overview" width="600"/>
</p>

---

## ğŸ“ˆ Real-Time Analysis Results

### ğŸ“ Text Analysis with Happiness Meter
- ğŸ“Š Shows emotion breakdown from text with visual feedback
- ğŸ§  Includes mental health indicators for better well-being tracking

<p align="center">
  <img src="ResultsOfProject/web3.png" alt="Text Emotion Analysis" width="700"/>
</p>

### ğŸ¤ Audio Analysis with Speedometer
- ğŸ”Š Detects voice emotion with confidence levels
- ğŸ¯ Displays results using a speedometer-style happiness gauge

<p align="center">
  <img src="ResultsOfProject/web4.png" alt="Audio Emotion Recognition" width="700"/>
</p>

### ğŸ¬ Video Analysis with Live Detection
- ğŸ“¹ Real-time facial expression analysis
- ğŸ“ˆ Tracks emotion frequency for comprehensive behavioral insight

<p align="center">
  <img src="ResultsOfProject/web5.png" alt="Video Emotion Detection" width="700"/>
</p>

---

## ğŸ—ï¸ Project Structure

```
MultiModalMentalHealth/
â”œâ”€â”€ ğŸ¨ MiniProject_Frontend/
â”‚   â””â”€â”€ emotion-detector-app/              # React Single Page Application
â”‚       â”œâ”€â”€ ğŸ“ src/
â”‚       â”‚   â”œâ”€â”€ ğŸ“Š FinalSummary/           # Results summary components
â”‚       â”‚   â”‚   â””â”€â”€ FinalSummary.jsx       # Overall analysis dashboard
â”‚       â”‚   â”œâ”€â”€ ğŸŒ GlobalAPIResults/       # API response handlers
â”‚       â”‚   â”‚   â””â”€â”€ GlobalAPIResults.jsx
â”‚       â”‚   â”œâ”€â”€ ğŸ¥ MentalHealth/           # Mental health analysis
â”‚       â”‚   â”‚   â””â”€â”€ MentalHealthPredictor.jsx
â”‚       â”‚   â”œâ”€â”€ ğŸ”§ Services/               # API service layer
â”‚       â”‚   â”‚   â”œâ”€â”€ EmotionTextAPI.jsx     # Text analysis API
â”‚       â”‚   â”‚   â”œâ”€â”€ EmotionVideoAPI.jsx    # Video analysis API
â”‚       â”‚   â”‚   â”œâ”€â”€ EmotionVoiceAPI.jsx    # Audio analysis API
â”‚       â”‚   â”‚   â””â”€â”€ MentalHealthAPI.jsx    # Mental health API
â”‚       â”‚   â”œâ”€â”€ ğŸ“ TextEmotionDetector/
â”‚       â”‚   â”‚   â””â”€â”€ TextEmotionDetector.jsx
â”‚       â”‚   â”œâ”€â”€ ğŸ¬ VideoEmotionPredictor/
â”‚       â”‚   â”‚   â””â”€â”€ VideoEmotionPredictor.jsx
â”‚       â”‚   â”œâ”€â”€ ğŸ¤ VoiceEmotionPredictor/
â”‚       â”‚   â”‚   â””â”€â”€ VoiceEmotionPredictor.jsx
â”‚       â”‚   â”œâ”€â”€ ğŸ¨ App.css                 # Main styling
â”‚       â”‚   â”œâ”€â”€ âš›ï¸ App.js                  # Main application component
â”‚       â”‚   â”œâ”€â”€ ğŸ§ª App.test.js             # Test configuration
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ index.css               # Global styles
â”‚       â”‚   â”œâ”€â”€ ğŸš€ index.js                # Application entry point
â”‚       â”‚   â”œâ”€â”€ ğŸ·ï¸ logo.svg                # Application logo
â”‚       â”‚   â”œâ”€â”€ ğŸ“Š reportWebVitals.js      # Performance monitoring
â”‚       â”‚   â””â”€â”€ ğŸ› ï¸ setupTests.js           # Testing setup
â”‚       â”œâ”€â”€ ğŸ“ public/                     # Static assets
â”‚       â”œâ”€â”€ ğŸ“¦ package.json                # Dependencies
â”‚       â”œâ”€â”€ âš™ï¸ tailwind.config.js          # TailwindCSS config
â”‚       â””â”€â”€ ğŸ” .env                        # Environment variables
â”œâ”€â”€ ğŸš€ Mini_Project_API/                   # FastAPI Backend
â”‚   â”œâ”€â”€ ğŸ¤– TextEmotionModel/               # BERT-based text models
â”‚   â”œâ”€â”€ ğŸ“ Textemotion/                    # Text processing utilities  
â”‚   â”œâ”€â”€ ğŸ¬ VideoEmotionModel/              # YOLOv8 video models
â”‚   â”œâ”€â”€ ğŸ§  textmentalhealth_92/            # Mental health assessment
â”‚   â”œâ”€â”€ ğŸ”Š voicemodel/                     # Audio emotion models
â”‚   â”œâ”€â”€ ğŸ¯ client.py                       # Development server launcher
â”‚   â”œâ”€â”€ ğŸ“Š emotion_text_prediction.py      # Text emotion endpoints
â”‚   â”œâ”€â”€ ğŸµ emotion_voice.py                # Audio emotion endpoints
â”‚   â”œâ”€â”€ ğŸ¥ mental_health_predictor.py      # Mental health endpoints
â”‚   â”œâ”€â”€ ğŸ¥ yolo_utils.py                   # Video processing utilities
â”‚   â”œâ”€â”€ ğŸšª main.py                         # FastAPI application
â”‚   â””â”€â”€ ğŸ“‹ requirements.txt                # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md                           # Project documentation
â””â”€â”€ ğŸ”§ Configuration files                 # Git and project configs
```

---

## ğŸš€ Easy Installation Guide

### ğŸ“‹ Prerequisites

Make sure you have the following installed before getting started:

- ğŸ **Python 3.8+**
- âš¡ **Node.js 16+** and **npm**
- ğŸ“¦ **Git** with **Git LFS** support
- ğŸ¥ğŸ¤ **A webcam and microphone** (for live emotion analysis)

### ğŸ”¥ Quick Start â€” Get Running in Under 5 Minutes!

#### Step 1: Clone the Repository

```bash
git clone https://github.com/yourusername/MultiModalMentalHealth.git
cd MultiModalMentalHealth
```

#### Step 2: Backend Setup (FastAPI)

```bash
# Navigate to backend directory
cd Mini_Project_API

# Create virtual environment
python -m venv venv

# Activate virtual environment
# For Windows:
venv\Scripts\activate

# For macOS/Linux:
source venv/bin/activate

# Install all dependencies
pip install -r requirements.txt

# Start the FastAPI server
python client.py
```

**âœ… Backend Ready!**  
Server will be live at: `http://localhost:8080`

#### Step 3: Frontend Setup (React)

```bash
# Open a new terminal and navigate to the frontend directory
cd MiniProject_Frontend/emotion-detector-app

# Install dependencies
npm install

# Create environment config
echo "REACT_APP_BASE_URL=http://localhost:8080" > .env

# Start the React application
npm start
```

**âœ… Frontend Ready!**  
Your app will open at: `http://localhost:3000`

### ğŸ¯ That's It! You're All Set!

1. Open your browser and visit `http://localhost:3000`
2. Start exploring emotions across text, audio, and video â€” all in real-time!

---

## ğŸ¯ How to Use the Application

### ğŸ–¥ï¸ Single-Page Interface
The application features a sleek, single-page design where all analysis tools â€” text, voice, and video â€” are integrated into one smooth user experience.

### ğŸ“ Text Emotion Analysis
1. **âœï¸ Input:** Type your thoughts, feelings, or any content in the text area
2. **ğŸ”˜ Click:** Hit the "Generate Assessment Report" button
3. **âš¡ View:** Instantly see emotion predictions and happiness meter
4. **ğŸ“Š Results:**
   - Primary emotion (e.g., joy, anger, sadness)
   - Confidence level
   - Mental health indicators

### ğŸ¤ Voice Emotion Recognition
1. **ğŸ§ Upload:** Click "Voice Emotion Detector" and upload an audio file (.wav format)
2. **ğŸ§  Analyze:** Press "Analyze Emotion" to start processing
3. **ğŸ“ˆ Results:**
   - Emotional breakdown with confidence scores
   - Visual speedometer showing happiness level

### ğŸ¬ Video-Based Emotion Detection
1. **ğŸ¥ Start:** Click "Start Detection" to turn on your camera
2. **ğŸ“· Live Feed:** Position yourself in front of the webcam
3. **ğŸ” Real-Time Analysis:**
   - Watch emotions detected live
   - See frequency counters for each emotion
   - Get a full visual report of facial expressions

### ğŸ“Š Understanding the Happiness Meter

The speedometer-style visualization helps interpret emotional well-being at a glance:

| Zone | Range | Description |
|------|-------|-------------|
| ğŸŸ¢ **Green** | 70â€“100% | High happiness, positive state |
| ğŸŸ¡ **Yellow** | 40â€“69% | Neutral state, balanced mood |
| ğŸ”´ **Red** | 0â€“39% | Low happiness, may need care |

### ğŸ”„ Multimodal Emotion Analysis
- **ğŸ§© Combine Inputs:** Use text, voice, and video together for deeper insights
- **ğŸ“Š Real-Time Dashboard:** View all emotions side-by-side in one unified panel
- **ğŸ“ Export Reports:** Generate detailed emotional health summaries â€” great for tracking or professional assessments

---

## ğŸ› ï¸ Technology Stack

Explore the powerful technologies that bring MultiModalMentalHealth to life â€” from modern frontend frameworks to cutting-edge AI libraries.

### ğŸ¨ Frontend Technologies
A clean and responsive single-page application (SPA) built for performance and usability:

- **âš›ï¸ React 18** â€” A fast, flexible JavaScript library for building dynamic UIs
- **ğŸ¨ TailwindCSS** â€” Utility-first CSS framework for fully responsive and elegant designs
- **âœ¨ JavaScript (ES6+)** â€” Modern JS with features like async/await, arrow functions, and destructuring
- **ğŸ§© Component Architecture** â€” Modular, reusable React components for each feature (Text, Audio, Video analysis)

### ğŸš€ Backend Technologies
A high-performance backend that handles real-time requests and processes media data efficiently:

- **âš¡ FastAPI** â€” Blazing-fast Python web framework with automatic Swagger docs
- **ğŸš€ Uvicorn** â€” ASGI server that powers the backend with ultra-low latency
- **ğŸŒ CORS Middleware** â€” Enables seamless communication between frontend and backend
- **ğŸ“ File Upload Handling** â€” Supports audio/video uploads with multipart form data

### ğŸ¤– AI & Machine Learning Technologies
The core intelligence of the app is powered by state-of-the-art AI libraries:

- **ğŸ§  HuggingFace Transformers** â€” Pre-trained BERT models for emotion analysis from text
- **ğŸµ TensorFlow** â€” Processes and classifies audio signals for emotion recognition
- **ğŸ”¥ PyTorch** â€” Powers neural network inference and model evaluation
- **ğŸ§ Librosa** â€” Extracts audio features like MFCCs, pitch, and zero-crossing rate
- **ğŸ‘ï¸ OpenCV** â€” Handles video capture and image preprocessing
- **ğŸ§¿ YOLOv8 (Ultralytics)** â€” Detects real-time facial expressions with high accuracy
- **ğŸ§ª Scikit-learn** â€” Offers utilities for preprocessing, evaluation, and model tuning

---

## ğŸ”§ Configuration & Environment Setup

This section helps you get your environment variables, dependencies, and configurations set up correctly â€” for both the frontend and backend.

### ğŸŒ Frontend Environment Variables

Create a `.env` file inside `MiniProject_Frontend/emotion-detector-app/`:

```env
# Backend API Base URL
REACT_APP_BASE_URL=http://localhost:8080

# For development with tunneling (optional)
# REACT_APP_BASE_URL=https://your-tunnel-url.com

# Enable development mode (optional)
REACT_APP_ENV=development
```

### âš™ï¸ Backend Configuration

The `client.py` script in `Mini_Project_API` automatically sets up your FastAPI server.

```python
# Automatic IP detection and server startup
HOST = "0.0.0.0"     # Accept connections from any IP
PORT = 8080          # Default port

# CORS settings (pre-configured for development)
CORS_ORIGINS = ["*"]  # Allow all origins
```

### ğŸ“¦ Dependencies Management

#### âœ… Python Backend Dependencies (requirements.txt)

Make sure these packages are installed in your backend virtual environment:

```txt
torch>=1.9.0
transformers>=4.20.0
safetensors>=0.3.0
tensorflow>=2.8.0
librosa>=0.9.0
soundfile>=0.10.0
scikit-learn>=1.1.0
fastapi>=0.85.0
uvicorn[standard]>=0.18.0
aiofiles>=0.8.0
python-multipart>=0.0.5
ultralytics>=8.0.0
opencv-python>=4.6.0
pydantic>=1.9.0
```

Install all dependencies:

```bash
pip install -r requirements.txt
```

#### âœ… React Frontend Dependencies

Make sure your frontend includes:
- React 18+ (with Hooks)
- TailwindCSS
- Axios (for API calls, if used)
- File upload utilities (e.g., input handlers)

Install dependencies with:

```bash
npm install
```

---

## ğŸ› Troubleshooting Guide

### ğŸ”´ Backend Won't Start

```bash
# Check if port 8080 is in use
netstat -ano | findstr :8080

# Kill the process if needed (Windows)
taskkill /PID <PID_NUMBER> /F
```

Or, change the port in `client.py`:

```python
PORT = 8081  # Use a different available port
```

### ğŸ”´ "Module Not Found" Errors

```bash
# Make sure your virtual environment is activated

# Windows:
venv\Scripts\activate

# macOS/Linux:
source venv/bin/activate

# Reinstall dependencies
pip install -r requirements.txt
```

### ğŸ”´ Frontend Can't Connect to Backend

```bash
# Check .env file path & value
cat MiniProject_Frontend/emotion-detector-app/.env

# Ensure backend server is running
curl http://localhost:8080
```

**âœ… Common Fixes:**
- Make sure backend is running on correct port
- Confirm `REACT_APP_BASE_URL` matches backend URL
- Temporarily disable firewalls blocking localhost

### ğŸ”´ Camera / Microphone Access Denied

**Chrome/Edge:** Click the ğŸ”’ or ğŸ¥ icon in the address bar â†’ Allow access

**Firefox:** Click the ğŸ›¡ï¸ shield â†’ Manage permissions â†’ Allow camera/mic

**System Settings:**
- **Windows:** Go to Privacy Settings â†’ Camera & Microphone â†’ Allow apps
- **macOS:** System Preferences â†’ Security & Privacy â†’ Enable access

**NOTE:** For production builds, camera/mic access requires HTTPS

---

## ğŸ Additional Troubleshooting & Tips

Get your environment back on track with these advanced fixes for common issues related to model files, builds, and performance.

### ğŸ”´ Large Model Files Missing

If some model folders are empty or not working correctly, it's likely due to missing Git Large File Storage (LFS) files.

```bash
# Step 1: Install Git LFS if you haven't already
git lfs install

# Step 2: Pull large model files
git lfs pull
```

**âœ… Verify the model directories contain files:**

```bash
ls Mini_Project_API/TextEmotionModel/
ls Mini_Project_API/VideoEmotionModel/
ls Mini_Project_API/voicemodel/
```

If folders are empty, the model weights haven't been downloaded properly. Re-run the above commands or check your GitHub permissions for LFS.

### ğŸ”´ React App Build or Start Issues

If the frontend refuses to start or throws strange errors:

```bash
# Step 1: Clear NPM cache
npm cache clean --force

# Step 2: Delete node_modules and lock file
rm -rf node_modules package-lock.json

# Step 3: Reinstall dependencies
npm install

# Step 4: Verify Node.js and npm versions
node --version   # Should be 16.x or higher
npm --version
```

**âš ï¸ Using incompatible versions of Node can break your build. Recommended: Node 16.x and npm 8+.**

---

## ğŸ› ï¸ Advanced Troubleshooting

### ğŸš€ Performance Issues

**Video Detection is Lagging?**
- Lower the camera resolution in your browser or use an external webcam with higher FPS.

**Model Takes Time to Respond?**
- Initial model loading may take 30â€“60 seconds after the first request.

**System Slow or Laggy?**
- Free up system memory by closing unnecessary apps, especially when running all 3 modalities (Text, Audio, Video) together.

### ğŸ’¡ Developer Tips

**ğŸ” Hot Reload Enabled**
- Frontend (`npm start`) and backend (`uvicorn`) both support hot reloading on file changes.

**ğŸ§ª Test API with Swagger UI**
- Open `http://localhost:8080/docs` to interact with all backend routes visually.

**ğŸ Enable Debug Mode**
- Set `REACT_APP_ENV=development` in the `.env` file for more detailed frontend error logs.

### ğŸ“ Still Need Help?

If you're still stuck or facing edge-case errors:

- **ğŸ§ª Check Browser Console:** Press F12 â†’ Console tab to inspect frontend issues
- **ğŸ–¥ï¸ Check Terminal Logs:** Look at FastAPI logs for backend exceptions
- **ğŸ› Create a GitHub Issue:**
  - Include a short description, terminal logs, error screenshots, and your system details

**ğŸ’» System Requirements:**
- Python 3.8+
- Node.js 16+
- Webcam

---

<div align="center">

**Made with â¤ï¸ for mental health awareness and AI innovation**

*Remember: Technology can support mental health, but human connection and professional care remain irreplaceable.*

</div>
